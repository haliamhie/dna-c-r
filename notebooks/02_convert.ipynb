{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ae7850",
   "metadata": {},
   "source": [
    "# 02 · Data Conversion: CSV → Parquet → DuckDB\n",
    "\n",
    "This notebook handles the conversion of raw CSV data to optimized Parquet format and creates a DuckDB database for fast queries.\n",
    "\n",
    "## Conversion Strategy\n",
    "1. **Stream Processing**: Read CSV in chunks to handle large file\n",
    "2. **Data Sanitization**: Clean and standardize barcode sequences\n",
    "3. **Partitioning**: Split into multiple Parquet files for parallel processing\n",
    "4. **Database Views**: Create DuckDB views for efficient querying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2cdc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "root = Path(\"/home/mch/dna\")\n",
    "sys.path.append(str(root / \"scripts\"))\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = root / \"DNA-Data for Telhai\" / \"2023-05-11\"\n",
    "ARTIFACTS = root / \"artifacts\"\n",
    "clusters_csv = DATA_DIR / \"clusters.csv\"\n",
    "parquet_dir = ARTIFACTS / \"clusters_parquet\"\n",
    "db_path = ARTIFACTS / \"dna.duckdb\"\n",
    "\n",
    "print(f\"Source CSV: {clusters_csv}\")\n",
    "print(f\"Target Parquet: {parquet_dir}\")\n",
    "print(f\"Target DB: {db_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c023eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Run the conversion script\n",
    "# Uncomment to run the full conversion (may take a few minutes)\n",
    "# !python /home/mch/dna/scripts/convert_to_parquet.py 2>&1 | tee /home/mch/dna/logs/conversion.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2105037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Manual conversion with progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def convert_csv_to_parquet(csv_path, parquet_dir, chunksize=500_000):\n",
    "    \"\"\"Convert CSV to Parquet with chunking and progress bar.\"\"\"\n",
    "    parquet_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Count total rows for progress bar\n",
    "    with open(csv_path, 'r') as f:\n",
    "        total_rows = sum(1 for _ in f) - 1\n",
    "    \n",
    "    chunks = pd.read_csv(csv_path, chunksize=chunksize)\n",
    "    \n",
    "    with tqdm(total=total_rows, desc=\"Converting\") as pbar:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Sanitize barcode column\n",
    "            if 'barcode' in chunk.columns:\n",
    "                chunk['barcode'] = (chunk['barcode']\n",
    "                                    .astype('string')\n",
    "                                    .str.upper()\n",
    "                                    .str.replace(r\"[^ACGT]\", \"\", regex=True))\n",
    "            \n",
    "            # Write to parquet\n",
    "            out_path = parquet_dir / f'part_{i:05d}.parquet'\n",
    "            chunk.to_parquet(out_path, index=False)\n",
    "            pbar.update(len(chunk))\n",
    "    \n",
    "    return i + 1  # Return number of parts created\n",
    "\n",
    "# Uncomment to run conversion\n",
    "# num_parts = convert_csv_to_parquet(clusters_csv, parquet_dir)\n",
    "# print(f\"Created {num_parts} parquet files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c6b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to DuckDB and create/verify views\n",
    "con = duckdb.connect(str(db_path))\n",
    "\n",
    "# Install and load parquet extension\n",
    "con.execute(\"INSTALL parquet; LOAD parquet;\")\n",
    "\n",
    "# Create main view\n",
    "con.execute(\n",
    "    \"CREATE OR REPLACE VIEW clusters AS SELECT * FROM read_parquet(?)\",\n",
    "    [str(parquet_dir / \"*.parquet\")]\n",
    ")\n",
    "\n",
    "# Verify the view\n",
    "result = con.sql(\"SELECT COUNT(*) as total_rows FROM clusters\")\n",
    "print(\"Database view created successfully!\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef712e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional analytical views\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW barcode_stats AS\n",
    "    SELECT \n",
    "        COUNT(*) as total_rows,\n",
    "        COUNT(DISTINCT barcode) as unique_barcodes,\n",
    "        COUNT(CASE WHEN barcode IS NULL OR barcode = '' THEN 1 END) as null_barcodes,\n",
    "        COUNT(CASE WHEN barcode IS NOT NULL AND barcode != '' THEN 1 END) as valid_barcodes\n",
    "    FROM clusters\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW barcode_lengths AS\n",
    "    SELECT \n",
    "        LENGTH(barcode) as barcode_length,\n",
    "        COUNT(*) as count\n",
    "    FROM clusters\n",
    "    WHERE barcode IS NOT NULL AND barcode != ''\n",
    "    GROUP BY 1\n",
    "    ORDER BY 2 DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Analytical views created:\")\n",
    "print(\"- barcode_stats: Overall statistics\")\n",
    "print(\"- barcode_lengths: Distribution of barcode lengths\")\n",
    "\n",
    "# Show stats\n",
    "con.sql(\"SELECT * FROM barcode_stats\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e19633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "con.close()\n",
    "print(\"Database setup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dnaenv)",
   "language": "python",
   "name": "dnaenv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
